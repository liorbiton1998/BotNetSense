{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "This notebook includes code developed and tested in Vertex AI Studio using Gemini 1.5 Pro (gemini-1.5-pro-002), a paid large language model by Google. It is now transferred to Google Colab for documentation and sharing purposes."
      ],
      "metadata": {
        "id": "Q92_JXCzVTuv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PyVWd1TmT6J2"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade google-cloud-aiplatform"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import base64\n",
        "import vertexai\n",
        "from vertexai.generative_models import GenerativeModel, Part, SafetySetting\n",
        "import pandas as pd\n",
        "from google.cloud import aiplatform, storage\n",
        "from vertexai import init\n",
        "from vertexai.generative_models import GenerativeModel, SafetySetting\n",
        "import csv\n",
        "import time\n",
        "from google.api_core.exceptions import ResourceExhausted\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Initialize Vertex AI environment\n",
        "init(project=\"---\", location=\"---\") #--> fill with your detailes\n",
        "\n",
        "# Load data\n",
        "file_path = \"/content/balanced_prompts_nov2022.csv\"\n",
        "\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Define the generation model and configuration\n",
        "model = GenerativeModel(\"gemini-1.5-pro-002\")\n",
        "generation_config = {\n",
        "    \"max_output_tokens\": 200,  # token size\n",
        "    \"temperature\": 0.7,\n",
        "    \"top_p\": 0.95,\n",
        "}\n",
        "safety_settings = [\n",
        "    SafetySetting(category=SafetySetting.HarmCategory.HARM_CATEGORY_HATE_SPEECH, threshold=SafetySetting.HarmBlockThreshold.OFF),\n",
        "    SafetySetting(category=SafetySetting.HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT, threshold=SafetySetting.HarmBlockThreshold.OFF),\n",
        "    SafetySetting(category=SafetySetting.HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT, threshold=SafetySetting.HarmBlockThreshold.OFF),\n",
        "    SafetySetting(category=SafetySetting.HarmCategory.HARM_CATEGORY_HARASSMENT, threshold=SafetySetting.HarmBlockThreshold.OFF),\n",
        "]\n",
        "\n",
        "prompt_columns = [\n",
        "    \"Minimal Prompt\",\n",
        "    \"Full-Feature Prompt\"\n",
        "]\n",
        "\n",
        "\n",
        "# Prepare CSV to store results in a single file\n",
        "local_csv_path = \"/content/gemini_responses_nov2022.csv\"\n",
        "gcs_bucket_name = \"\"  # <-- Replace to bucket name\n",
        "gcs_blob_name = \"gemini/gemini_responses_nov2022.csv\"\n",
        "\n",
        "def generate_with_backoff(prompt, prompt_type, retries=2, delay=10):\n",
        "    # Choose generation config based on prompt type\n",
        "    if \"Chain\" in prompt_type or \"CoT\" in prompt_type:\n",
        "        config = {\"temperature\": 0.7, \"top_p\": 0.95, \"max_output_tokens\": 200}\n",
        "    else:\n",
        "        config = {\"temperature\": 0.2, \"top_p\": 0.8, \"max_output_tokens\": 20}\n",
        "    for attempt in range(retries):\n",
        "        try:\n",
        "            responses = model.generate_content(\n",
        "                [prompt],\n",
        "                generation_config=config,\n",
        "                safety_settings=safety_settings,\n",
        "                stream=True,\n",
        "            )\n",
        "            response_text = \"\".join([response.text for response in responses])\n",
        "            return response_text\n",
        "        except ResourceExhausted:\n",
        "            print(f\"Quota exceeded. Retrying in {delay} seconds... (Attempt {attempt + 1}/{retries})\")\n",
        "            time.sleep(delay)\n",
        "            delay += 5\n",
        "    return \"Error: Quota exceeded after retries\"\n",
        "\n",
        "with open(local_csv_path, mode=\"w\", newline=\"\", encoding=\"utf-8-sig\") as f:\n",
        "    writer = csv.writer(f)\n",
        "    header = [\"user_id\", \"tweet_id\", \"is_bot\"] + [f\"{col} Response\" for col in prompt_columns]\n",
        "    writer.writerow(header)\n",
        "\n",
        "    batch_size = 24\n",
        "    batch_count = (len(data) + batch_size - 1) // batch_size\n",
        "\n",
        "    for i in tqdm(range(0, len(data), batch_size), desc=\"Processing Batches\"):\n",
        "        batch = data.iloc[i:i + batch_size]\n",
        "        batch_index = i // batch_size + 1\n",
        "\n",
        "        with tqdm(total=len(batch), desc=f\"Batch {batch_index}/{batch_count} Tweets\", leave=False) as pbar_tweets:\n",
        "            for _, row in batch.iterrows():\n",
        "                user_id = row[\"user_id\"]\n",
        "                tweet_id = row[\"tweet_id\"]\n",
        "                bot_label = row[\"is_bot\"]\n",
        "                responses_for_row = [user_id, tweet_id, bot_label]\n",
        "\n",
        "                for column in prompt_columns:\n",
        "                    prompt = row[column]\n",
        "                    if pd.isna(prompt):\n",
        "                        response_text = \"No prompt available\"\n",
        "                    else:\n",
        "                        response_text = generate_with_backoff(prompt, column)\n",
        "                        time.sleep(1)\n",
        "                    responses_for_row.append(response_text)\n",
        "\n",
        "                writer.writerow(responses_for_row)\n",
        "                pbar_tweets.update(1)\n",
        "\n",
        "        print(f\"Batch {batch_index} completed. Pausing for 10 seconds...\")\n",
        "        time.sleep(10)\n",
        "\n",
        "print(f\"CSV written to: {local_csv_path}\")\n",
        "\n",
        "# Upload to Google Cloud Storage\n",
        "def upload_to_gcs(local_path, bucket_name, destination_blob_name):\n",
        "    client = storage.Client()\n",
        "    bucket = client.bucket(bucket_name)\n",
        "    blob = bucket.blob(destination_blob_name)\n",
        "    blob.upload_from_filename(local_path)\n",
        "    print(f\"Uploaded to GCS: gs://{bucket_name}/{destination_blob_name}\")\n",
        "\n",
        "upload_to_gcs(local_csv_path, gcs_bucket_name, gcs_blob_name)"
      ],
      "metadata": {
        "id": "ynViwR5IUHeK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}